{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import img_as_float\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import adam\n",
    "from keras.utils import conv_utils\n",
    "from keras.engine.topology import Layer\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, PReLU, BatchNormalization, Add, LeakyReLU, Dense, Flatten\n",
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/home/ms3u14/COMP6208-AutoEncoder/several27/data/'\n",
    "path_div2k_train_hr = path_data + 'DIV2K_train_HR/'\n",
    "path_div2k_train_lr = path_data + 'DIV2K_train_LR_bicubic/X4/'\n",
    "path_div2k_test_hr = path_data + 'DIV2K_valid_HR/'\n",
    "path_div2k_test_lr = path_data + 'DIV2K_valid_LR_bicubic/X4/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Other useful dataset: \n",
    "- https://data.vision.ee.ethz.ch/cvl/DIV2K/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - 0.5) / 0.5\n",
    "\n",
    "def denorm(x):\n",
    "    return x * 0.5 + 0.5\n",
    "\n",
    "def norm_to_img(x):\n",
    "    return Image.fromarray((denorm(x) * 255).astype(np.uint8))\n",
    "\n",
    "def img_to_norm(x):\n",
    "    return norm(img_as_float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "with tqdm() as progress:\n",
    "    for file in os.listdir(path_div2k_train_hr):\n",
    "        if file.endswith('png'):\n",
    "            x_train.append(img_to_norm(Image.open(path_div2k_train_hr + file)))\n",
    "            \n",
    "        progress.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {}\n",
    "for x in x_train:\n",
    "    counter.setdefault(x.shape[0], 0)\n",
    "    counter[x.shape[0]] += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = (img_as_float(x_train[0]) - 0.5) / 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_to_img(x_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow((x_train_norm * x_train_std + x_train_mean).astype(np.uint8)[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_images(path, size=(420, 560, 3), ratio=2, batch_size=32):\n",
    "    lr_height, lr_width = size[0] // ratio, size[1] // ratio\n",
    "    \n",
    "    batch_i = 0\n",
    "    batch = np.zeros((batch_size, size[0], size[1], size[2]))\n",
    "    batch_scaled = np.zeros((batch_size, lr_height, lr_width, size[2]))\n",
    "    \n",
    "    while True:\n",
    "        for file in os.listdir(path):\n",
    "            if not file.endswith('.jpg'):\n",
    "                continue \n",
    "            \n",
    "            if batch_i == batch_size:\n",
    "                yield batch_scaled, batch\n",
    "                \n",
    "                batch_i = 0\n",
    "                batch = np.zeros((batch_size, size[0], size[1], size[2]))\n",
    "                batch_scaled = np.zeros((batch_size, lr_height, lr_width, size[2]))\n",
    "            \n",
    "            file_path = path + file\n",
    "            img = img_as_float(Image.open(file_path))\n",
    "            if len(img.shape) == 2: \n",
    "                 img = np.asarray(np.dstack((img, img, img))) \n",
    "            \n",
    "            batch[batch_i] = img\n",
    "            batch_scaled[batch_i] = resize(img, (lr_height, lr_width))\n",
    "            \n",
    "            batch_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(path):\n",
    "    return sum([1 for file in os.listdir(path) if file.endswith('.jpg')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images(path_open_images_560_420_train), count_images(path_open_images_560_420_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](srgan_architecture.png)\n",
    "\n",
    "Useful links: \n",
    "- https://github.com/liangstein/SRGAN-Keras/blob/master/py.py\n",
    "- https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks/blob/master/models.py\n",
    "- https://github.com/soumith/ganhacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator network\n",
    "\n",
    "1. Input - Image LR\n",
    "2. Conv, kernel 3x3, 64 feature maps, what's s1?\n",
    "3. ParametricReLU\n",
    "4. B=16 residual blocks\n",
    "    1. Conv with 3x3 kernel, 64 feature maps and stride 1\n",
    "    2. Batch Normalization (https://keras.io/layers/normalization/)\n",
    "    3. ParametricReLU\n",
    "    4. Conv with 3x3 kernel, 64 feature maps and stride 1\n",
    "    5. Batch Normalization\n",
    "    6. Elementwise sum (https://keras.io/layers/merge/ add)\n",
    "5. Conv with 3x3 kernel, 64 feature maps and stride 1\n",
    "6. Batch Normalization\n",
    "7. Elementwise sum (https://keras.io/layers/merge/ add)\n",
    "8. Shuffle block x2 (? how it's called ?)\n",
    "    1. Conv with 3x3 kernel, 256 feature maps and stride 1\n",
    "    2. PixelShuffler x2 https://gist.github.com/t-ae/6e1016cc188104d123676ccef3264981\n",
    "    3. ParametricReLU\n",
    "9. Conv with 9x9 kernel, 3 feature maps and stride 1\n",
    "10. Estimated SR (super-resolved) image\n",
    "\n",
    "Is the padding `same` a correct one? Otherwise the dims are not the same\n",
    "Is the number of pixel shufflers the scaling ratio? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://gist.github.com/t-ae/6e1016cc188104d123676ccef3264981\n",
    "\n",
    "class PixelShuffler(Layer):\n",
    "    def __init__(self, size=(2, 2), data_format=None, **kwargs):\n",
    "        super(PixelShuffler, self).__init__(**kwargs)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        if len(input_shape) != 4:\n",
    "            raise ValueError('Inputs should have rank ' +\n",
    "                             str(4) +\n",
    "                             '; Received input shape:', str(input_shape))\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_size, c, h, w = input_shape\n",
    "            if batch_size is None:\n",
    "                batch_size = -1\n",
    "            rh, rw = self.size\n",
    "            oh, ow = h * rh, w * rw\n",
    "            oc = c // (rh * rw)\n",
    "\n",
    "            out = K.reshape(inputs, (batch_size, rh, rw, oc, h, w))\n",
    "            out = K.permute_dimensions(out, (0, 3, 4, 1, 5, 2))\n",
    "            out = K.reshape(out, (batch_size, oc, oh, ow))\n",
    "            return out\n",
    "\n",
    "        elif self.data_format == 'channels_last':\n",
    "            batch_size, h, w, c = input_shape\n",
    "            if batch_size is None:\n",
    "                batch_size = -1\n",
    "            rh, rw = self.size\n",
    "            oh, ow = h * rh, w * rw\n",
    "            oc = c // (rh * rw)\n",
    "\n",
    "            out = K.reshape(inputs, (batch_size, h, w, rh, rw, oc))\n",
    "            out = K.permute_dimensions(out, (0, 1, 3, 2, 4, 5))\n",
    "            out = K.reshape(out, (batch_size, oh, ow, oc))\n",
    "            return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        if len(input_shape) != 4:\n",
    "            raise ValueError('Inputs should have rank ' +\n",
    "                             str(4) +\n",
    "                             '; Received input shape:', str(input_shape))\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            height = input_shape[2] * self.size[0] if input_shape[2] is not None else None\n",
    "            width = input_shape[3] * self.size[1] if input_shape[3] is not None else None\n",
    "            channels = input_shape[1] // self.size[0] // self.size[1]\n",
    "\n",
    "            if channels * self.size[0] * self.size[1] != input_shape[1]:\n",
    "                raise ValueError('channels of input and size are incompatible')\n",
    "\n",
    "            return (input_shape[0],\n",
    "                    channels,\n",
    "                    height,\n",
    "                    width)\n",
    "\n",
    "        elif self.data_format == 'channels_last':\n",
    "            height = input_shape[1] * self.size[0] if input_shape[1] is not None else None\n",
    "            width = input_shape[2] * self.size[1] if input_shape[2] is not None else None\n",
    "            channels = input_shape[3] // self.size[0] // self.size[1]\n",
    "\n",
    "            if channels * self.size[0] * self.size[1] != input_shape[3]:\n",
    "                raise ValueError('channels of input and size are incompatible')\n",
    "\n",
    "            return (input_shape[0],\n",
    "                    height,\n",
    "                    width,\n",
    "                    channels)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'size': self.size,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(PixelShuffler, self).get_config()\n",
    "\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srgan_generator(input_shape, input_=None):\n",
    "    kernel_size = (3, 3)\n",
    "    kernel_size_last = (9, 9)\n",
    "    features = 64\n",
    "    features_shuffle = 256\n",
    "    features_last = 3\n",
    "    B = 16\n",
    "\n",
    "    # 1, 2, 3\n",
    "    input_1 = input_ if input_ is not None else Input(shape=input_shape)\n",
    "    conv2d_2 = Conv2D(filters=features, kernel_size=kernel_size, strides=(1, 1), padding='same', \n",
    "                      name='generator_conv2d_2')(input_1)\n",
    "    prelu_3 = PReLU(name='generator_prelu_3')(conv2d_2)\n",
    "\n",
    "    # 4 - residual blocks\n",
    "    last_layer = prelu_3\n",
    "    for i in range(B):\n",
    "        conv2d_4_A = Conv2D(filters=features, kernel_size=kernel_size, strides=(1, 1), padding='same', \n",
    "                            name='generator_conv2d_4_A_%s' % i)(last_layer)\n",
    "        bn_4_B = BatchNormalization(name='generator_bn_4_B_%s' % i)(conv2d_4_A)\n",
    "        prelu_4_C = PReLU(name='generator_prelu_4_C_%s' % i)(bn_4_B)\n",
    "        conv2d_4_D = Conv2D(filters=features, kernel_size=kernel_size, strides=(1, 1), padding='same', \n",
    "                            name='generator_conv2d_4_D_%s' % i)(prelu_4_C)\n",
    "        bn_4_E = BatchNormalization(name='generator_bn_4_E_%s' % i)(conv2d_4_D)\n",
    "        add_4_F = Add(name='generator_add_4_F_%s' % i)([last_layer, bn_4_E])\n",
    "\n",
    "        last_layer = add_4_F\n",
    "\n",
    "    # 5, 6, 7\n",
    "    conv2d_5 = Conv2D(filters=features, kernel_size=kernel_size, strides=(1, 1), padding='same', \n",
    "                      name='generator_conv2d_5')(last_layer)\n",
    "    bn_6 = BatchNormalization(name='generator_bn_6')(conv2d_5)\n",
    "    add_7 = Add(name='generator_add_7')([prelu_3, bn_6])\n",
    "\n",
    "    # 8 - shuffle block\n",
    "    last_layer = add_7\n",
    "    for i in range(1):\n",
    "        conv2d_8_A = Conv2D(filters=features_shuffle, kernel_size=kernel_size, strides=(1, 1), padding='same', \n",
    "                            name='generator_conv2d_8_A_%s' % i)(last_layer)\n",
    "        shuffler_8_B = PixelShuffler(name='generator_shuffler_8_B_%s' % i)(conv2d_8_A)\n",
    "        prelu_8_C = PReLU(name='generator_prelu_8_C_%s' % i)(shuffler_8_B)\n",
    "\n",
    "        last_layer = prelu_8_C\n",
    "\n",
    "    # 9 \n",
    "    conv2d_9 = Conv2D(filters=features_last, kernel_size=kernel_size_last, strides=(1, 1), \n",
    "                      padding='same', activation='tanh', name='generator_conv2d_9')(last_layer)\n",
    "\n",
    "    return Model(input_1, conv2d_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = srgan_generator((x_train_lr.shape[1], x_train_lr.shape[2], x_train_lr.shape[3]))\n",
    "# model_generator = srgan_generator((210, 280, 3))\n",
    "model_generator.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='data/srgan_generator_weights_%s.{epoch:03d}_{val_acc:.4f}.hdf5' % train_version, \n",
    "                               verbose=1, save_best_only=False)\n",
    "tb_callback = TensorBoard(log_dir='data/tensorboard/', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.fit(x_train_lr_norm, x_train_norm, epochs=1, validation_data=(x_test_lr_norm, x_test_norm), \n",
    "                    callbacks=[tb_callback])\n",
    "\n",
    "# batch_size = 1\n",
    "# n_train = 30 # count_images(path_open_images_560_420_train)\n",
    "# n_val = 3 # count_images(path_open_images_560_420_val)\n",
    "# with tf.device('/gpu:0'):\n",
    "#     model_generator.fit_generator(generator_images(path_open_images_560_420_train, batch_size=batch_size), \n",
    "#                                   steps_per_epoch=n_train // batch_size,\n",
    "#                                   validation_data=generator_images(path_open_images_560_420_val, \n",
    "#                                                                    batch_size=batch_size),\n",
    "#                                   validation_steps=n_val // batch_size, epochs=1, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.save('data/srgan_generator_%s.model' % train_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.load_weights('data/srgan_generator_%s.model' % train_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g, type_ in [(generator_images(path_open_images_560_420_train, batch_size=1), 'train'), \n",
    "                 (generator_images(path_open_images_560_420_val, batch_size=batch_size), 'test')]:\n",
    "    plt.figure(type_, figsize=(15, 30))\n",
    "    for i, (x_lr, x) in enumerate(g):\n",
    "        x_lr_ = (x_lr[0] * 255).astype(np.uint8)\n",
    "        x_ = (x[0] * 255).astype(np.uint8)\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 1)\n",
    "        plt.imshow(x_lr_)\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 2)\n",
    "        plt.imshow(x_)\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 3)\n",
    "        plt.imshow((model_generator.predict(x_lr)[0] * 255).astype(np.uint8))\n",
    "\n",
    "        if i >= 9:\n",
    "            break\n",
    "            \n",
    "    plt.savefig('data/srgan_generator_%s_%s.png' % (train_version, type_))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, x_lr, type_ in [(x_train, x_train_lr_norm, 'train'), (x_test, x_test_lr_norm, 'test')]:\n",
    "    plt.figure(figsize=(15, 30))\n",
    "    for i in range(10):\n",
    "        plt.subplot(10, 3, (i * 3) + 1)\n",
    "        plt.imshow(((x_lr[i] * x_train_lr_std + x_train_lr_mean) * 255).astype(np.uint8))\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 2)\n",
    "        plt.imshow(x[i])\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 3)\n",
    "        h, w = x_lr.shape[1], x_lr.shape[2]\n",
    "        prediction = model_generator.predict(x_lr[i].reshape(1, h, w, 3))[0]\n",
    "        plt.imshow((prediction * x_train_std + x_train_mean).astype(np.uint8))\n",
    "\n",
    "    plt.savefig('data/srgan_generator_%s_%s.png' % (train_version, type_))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 30))\n",
    "for i in range(10):\n",
    "    plt.subplot(10, 3, (i * 3) + 1)\n",
    "    plt.imshow(x_test_lr[i])\n",
    "\n",
    "    plt.subplot(10, 3, (i * 3) + 2)\n",
    "    plt.imshow(x_test[i])\n",
    "\n",
    "    plt.subplot(10, 3, (i * 3) + 3)\n",
    "    w, h = x_test_lr.shape[1], x_test_lr.shape[2]\n",
    "    plt.imshow(np.abs(model_generator.predict(x_test_lr[i].reshape(1, w, h, 3))[0]))\n",
    "\n",
    "plt.savefig('data/srgan_generator_%s_test.png' % train_version)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator network \n",
    "\n",
    "1. Input image\n",
    "2. Conv2d with 3x3 kernel, 64 filters, and 1 stride\n",
    "3. Leaky ReLU (https://keras.io/layers/advanced-activations/#leakyrelu)\n",
    "3. Conv2d block\n",
    "    1. Conv2d with 3x3 kernel, 64 filters, and 1 stride\n",
    "    2. Batch normalization\n",
    "    3. Leaky ReLU (https://keras.io/layers/advanced-activations/#leakyrelu)\n",
    "4. Repeat 4 with 128 filters x 2 \n",
    "5. Repeat 4 with 256 filters x 2 \n",
    "6. Repeat 4 with 512 filters x 2 \n",
    "7. Dense layer with 1024\n",
    "8. Leaky ReLU\n",
    "9. Dense 1 binary\n",
    "10. Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srgan_discriminator(input_prediction, input_original):\n",
    "    input_shape = 32, 32, 3\n",
    "    features_1 = 64\n",
    "    features_2, features_3, features_4 = 128, 256, 512\n",
    "    kernel_size = 3, 3\n",
    "    strides = 1, 1\n",
    "    strides_2 = 2, 2\n",
    "    \n",
    "    input_1 = Concatenate()([input_prediction, input_original])\n",
    "        \n",
    "    conv2d_2 = Conv2D(filters=features_1, kernel_size=kernel_size, strides=strides, padding='same', \n",
    "                      name='discriminator_conv2d_2')(input_1)\n",
    "    lrelu_3 = LeakyReLU(name='discriminator_lrelu_3')(conv2d_2)\n",
    "    \n",
    "    conv2d_4_A = Conv2D(filters=features_1, kernel_size=kernel_size, strides=strides_2, padding='same', \n",
    "                        name='discriminator_conv2d_4_A')(lrelu_3)\n",
    "    conv2d_4_B = BatchNormalization(name='discriminator_conv2d_4_B')(conv2d_4_A)\n",
    "    lrelu_4_c = LeakyReLU(name='discriminator_lrelu_4_c')(conv2d_4_B)\n",
    "\n",
    "    i = 0\n",
    "    last_layer = lrelu_4_c\n",
    "    for _features in [features_2, features_3, features_4]:\n",
    "        for j in range(2):\n",
    "            conv2d_5_A = Conv2D(filters=_features, kernel_size=kernel_size, strides=(strides if j == 0 else strides_2), \n",
    "                                padding='same', name='discriminator_conv2d_5_A_%s' % i)(last_layer)\n",
    "            conv2d_5_B = BatchNormalization(name='discriminator_conv2d_5_B_%s' % i)(conv2d_5_A)\n",
    "            lrelu_5_C = LeakyReLU(name='discriminator_lrelu_5_C_%s' % i)(conv2d_5_B)\n",
    "            \n",
    "            last_layer = lrelu_5_C\n",
    "            i += 1\n",
    "    \n",
    "    flatten_8 = Flatten(name='discriminator_flatten_8')(last_layer)\n",
    "    dense_8 = Dense(1024, name='discriminator_dense_8')(flatten_8)\n",
    "    lrelu_9 = LeakyReLU(name='discriminator_lrelu_9')(dense_8)\n",
    "    dense_10 = Dense(1, activation='sigmoid', name='discriminator_dense_10')(lrelu_9)\n",
    "    \n",
    "    return Model([input_prediction, input_original], dense_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_generator_cifar(model_generator, x, x_predictions, batch_size=64):        \n",
    "    batch_size = batch_size // 2\n",
    "    \n",
    "    batch_i = 0\n",
    "    batch = np.zeros((batch_size, 32, 32, 3))\n",
    "    batch_predictions = np.zeros((batch_size, 32, 32, 3))\n",
    "    \n",
    "    while True:\n",
    "        for i in range(x.shape[0]):\n",
    "            if batch_i == batch_size:\n",
    "                \n",
    "                # TODO: use soft and noisy labels\n",
    "                yield [np.concatenate((batch_predictions, batch)), np.concatenate((batch, batch))], \\\n",
    "                    np.concatenate((np.zeros(batch_size), np.ones(batch_size)))\n",
    "                \n",
    "                batch_i = 0\n",
    "                batch = np.zeros((batch_size, 32, 32, 3))\n",
    "                batch_predictions = np.zeros((batch_size, 32, 32, 3))\n",
    "            \n",
    "            batch[batch_i] = x[i]\n",
    "            batch_predictions[batch_i] = x_predictions[i]\n",
    "            \n",
    "            batch_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "train_version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_in = Input(shape=(16, 16, 3))\n",
    "\n",
    "model_generator = srgan_generator((16, 16, 3)) #, input_=generator_in)\n",
    "\n",
    "discriminator_input_prediction = Input(shape=(32, 32, 3))\n",
    "discriminator_input_original = Input(shape=(32, 32, 3))\n",
    "model_discriminator = srgan_discriminator(discriminator_input_prediction, discriminator_input_original)\n",
    "\n",
    "# generator_out = model_generator(generator_in)\n",
    "\n",
    "# discriminator_out = model_discriminator(generator_out)\n",
    "# model_srgan = Model(generator_in, discriminator_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.compile(optimizer='adam', loss='mse')\n",
    "model_discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# TODO try using softmax instead of sigmoid for categorical crossentropy\n",
    "# model_srgan.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.load_weights('data/srgan_generator_%s.model' % train_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_predictions = path_data + ('srgan_generator_%s_train_predictions.npy' % train_version)\n",
    "path_test_predictions = path_data + ('srgan_generator_%s_test_predictions.npy' % train_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predictions = model_generator.predict(x_train_lr_norm)\n",
    "x_test_predictions = model_generator.predict(x_test_lr_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(x_train_predictions), np.max(x_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY FIX OF GPU MEMORY LEAK ? \n",
    "\n",
    "np.save(path_train_predictions, x_train_predictions)\n",
    "np.save(path_test_predictions, x_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predictions = np.load(path_train_predictions)\n",
    "x_test_predictions = np.load(path_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='data/srgan_discriminator_weights_%s.{epoch:03d}_{val_acc:.4f}.hdf5' % \n",
    "                               train_version, verbose=1, save_best_only=False)\n",
    "tb_callback = TensorBoard(log_dir='data/tensorboard/', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_generator.fit(x_train_lr, x_train, epochs=100, validation_data=(x_test_lr, x_test), callbacks=[tb_callback])\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model_discriminator.fit_generator(discriminator_generator_cifar(model_generator, x_train_norm, x_train_predictions, \n",
    "                                                                    batch_size), \n",
    "                                      steps_per_epoch=x_train.shape[0] // (batch_size * 2), \n",
    "                                      validation_data=discriminator_generator_cifar(model_generator, \n",
    "                                                                                    x_test_norm, x_test_predictions, \n",
    "                                                                                    batch_size), \n",
    "                                      validation_steps=x_test.shape[0] // (batch_size * 2), epochs=epochs, \n",
    "                                      callbacks=[tb_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_discriminator.save('data/srgan_discriminator_%s.model' % train_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_discriminator.predict([model_generator.predict(x_train_lr_norm[2].reshape(1, 16, 16, 3)),\n",
    "                             x_train_norm[2].reshape(1, 32, 32, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train full SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks/blob/master/models.py#L420\n",
    "# What shall I do? \n",
    "\n",
    "# generator_in = Input(shape=(16, 16, 3))\n",
    "# discriminator_in = Input(shape=(32, 32, 16, 3))\n",
    "\n",
    "# model_generator = srgan_generator((16, 16, 3), input_=generator_in)\n",
    "# model_discriminator = srgan_discriminator()\n",
    "\n",
    "# generator_out = model_generator(generator_in)\n",
    "\n",
    "# discriminator_out = model_discriminator(generator_out)\n",
    "# model_srgan = Model(generator_in, discriminator_out)\n",
    "\n",
    "generator_in = Input(shape=(16, 16, 3))\n",
    "model_generator = srgan_generator((16, 16, 3), input_=generator_in)\n",
    "\n",
    "discriminator_in_predicted = Input(shape=(32, 32, 3))\n",
    "discriminator_in_original = Input(shape=(32, 32, 3))\n",
    "model_discriminator = srgan_discriminator(discriminator_in_predicted, discriminator_in_original)\n",
    "\n",
    "generator_out = model_generator(generator_in)\n",
    "discriminator_out = model_discriminator([generator_out, discriminator_in_original])\n",
    "model_srgan = Model([generator_in, discriminator_in_original], outputs=discriminator_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model_discriminator.compile(optimizer=adam(lr=0.00002), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_srgan.compile(optimizer=adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_srgan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 256\n",
    "train_version = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.load_weights(path_data + ('srgan_generator_%s.model' % train_version))\n",
    "model_discriminator.load_weights(path_data + ('srgan_discriminator_%s.model' % train_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='data/srgan_weights_%s.{epoch:03d}_{val_acc:.4f}.hdf5' % \n",
    "                               train_version, verbose=1, save_best_only=False)\n",
    "tb_callback = TensorBoard(log_dir='data/tensorboard/', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_generator.fit(x_train_lr, x_train, epochs=100, validation_data=(x_test_lr, x_test), callbacks=[tb_callback])\n",
    "\n",
    "# discriminative network should be online trained while training whole GAN as well, so that \n",
    "# it improves on differating the original vs superscaled\n",
    "\n",
    "def set_trainable(model, key_word, value=True):\n",
    "    layers_list = [layer for layer in model.layers if key_word in layer.name]\n",
    "    for layer in layers_list:\n",
    "        layer.trainable = value\n",
    "\n",
    "metrics_names = model_discriminator.metrics_names\n",
    "with tf.device('/gpu:0'):\n",
    "    for epoch in range(epochs):\n",
    "        print()\n",
    "        print('Epoch %s / %s' % (epoch, epochs))\n",
    "        \n",
    "        with tqdm(total=x_train_lr.shape[0] // batch_size) as progress:\n",
    "            for batch_i in np.arange(0, x_train_lr.shape[0], batch_size):\n",
    "                if batch_size + batch_i > x_train_lr.shape[0]:\n",
    "                    continue\n",
    "\n",
    "                x_train_batch = x_train_norm[batch_i:batch_i+batch_size]\n",
    "                x_train_lr_batch = x_train_lr_norm[batch_i:batch_i+batch_size]\n",
    "                x_train_predictions = model_generator.predict(x_train_lr_batch)\n",
    "\n",
    "                set_trainable(model_discriminator, 'discriminator', False)\n",
    "                metrics_srgan = model_srgan.train_on_batch([x_train_lr_batch, x_train_batch], [1] * batch_size)\n",
    "            \n",
    "                set_trainable(model_discriminator, 'discriminator', True)\n",
    "                metrics = model_discriminator.train_on_batch([np.concatenate([x_train_predictions, x_train_batch]),\n",
    "                                                              np.concatenate([x_train_batch, x_train_batch])], \n",
    "                                                             np.concatenate([[0] * batch_size, [1] * batch_size]))\n",
    "                progress.set_description('Srgan: %s: %s; %s: %s; Discriminator: %s: %s; %s: %s' % \n",
    "                    (metrics_names[0], metrics_srgan[0], metrics_names[1], metrics_srgan[1], metrics_names[0], \n",
    "                     metrics[0], metrics_names[1], metrics[1]))\n",
    "                progress.update()\n",
    "\n",
    "            eval_srgan = model_srgan.evaluate([x_test_lr_norm, x_test_norm], [1] * x_test_lr_norm.shape[0], verbose=0)\n",
    "            print('Model SRGAN evaluation: %s: %s; %s: %s;' % (metrics_names[0], eval_srgan[0], metrics_names[1], \n",
    "                                                               eval_srgan[1]))\n",
    "            \n",
    "            eval_g = model_generator.evaluate(x_test_lr_norm, x_test_norm, verbose=0)\n",
    "            print('Model generator evaluation: %s: %s; %s: %s' % (metrics_names[0], eval_g[0], metrics_names[1], \n",
    "                                                                  eval_g[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_generator.predict(x_train_lr_norm[:1])\n",
    "\n",
    "display(Image.fromarray(x_train[0]).resize((128, 128)))\n",
    "display(Image.fromarray((x_train_lr[0] * 255).astype(np.uint8)).resize((128, 128)))\n",
    "display(Image.fromarray((predictions[0] * x_train_std + x_train_mean).astype(np.uint8)).resize((128, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_srgan.evaluate(x_test_lr, [1] * x_test_lr.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_srgan.save(path_data + ('srgan_%s.model' % train_version))\n",
    "model_generator.save(path_data + ('srgan_final_generator_%s.model' % train_version))\n",
    "model_discriminator.save(path_data + ('srgan_final_discriminator_%s.model' % train_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, x_lr, type_ in [(x_train, x_train_lr_norm, 'train'), (x_test, x_test_lr_norm, 'test')]:\n",
    "    plt.figure(figsize=(15, 30))\n",
    "    for i in range(10):\n",
    "        plt.subplot(10, 3, (i * 3) + 1)\n",
    "        plt.imshow(((x_lr[i] * x_train_lr_std + x_train_lr_mean) * 255).astype(np.uint8))\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 2)\n",
    "        plt.imshow(x[i])\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 3)\n",
    "        h, w = x_lr.shape[1], x_lr.shape[2]\n",
    "        prediction = model_generator.predict(x_lr[i].reshape(1, h, w, 3))[0]\n",
    "        plt.imshow((prediction * x_train_std + x_train_mean).astype(np.uint8))\n",
    "\n",
    "    plt.savefig('data/srgan_final_%s_%s.png' % (train_version, type_))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image.fromarray(np.uint8((prediction - np.min(prediction))*255/(np.max(prediction) - np.min(prediction))))\\\n",
    "        .resize((128, 128)))\n",
    "display(Image.fromarray((prediction * x_train_std + x_train_mean).astype(np.uint8)).resize((128, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_generator.fit(x_train_lr, x_train, epochs=100, validation_data=(x_test_lr, x_test), callbacks=[tb_callback])\n",
    "\n",
    "# discriminative network should be online trained while training whole GAN as well, so that \n",
    "# it improves on differating the original vs superscaled\n",
    "\n",
    "def set_trainable(model, key_word, value=True):\n",
    "    layers_list = [layer for layer in model.layers if key_word in layer.name]\n",
    "    for layer in layers_list:\n",
    "        layer.trainable = value\n",
    "\n",
    "metrics_names = model_discriminator.metrics_names\n",
    "with tf.device('/gpu:0'):\n",
    "    for _ in range(30):\n",
    "        with tqdm(total=x_train_lr.shape[0] // batch_size) as progress:\n",
    "            for batch_i in np.arange(0, x_train_lr.shape[0], batch_size):\n",
    "                if batch_size + batch_i > x_train_lr.shape[0]:\n",
    "                    continue\n",
    "\n",
    "                x_train_batch = x_train_norm[batch_i:batch_i+batch_size]\n",
    "                x_train_lr_batch = x_train_lr_norm[batch_i:batch_i+batch_size]\n",
    "\n",
    "    #             set_trainable(model_generator, 'generator', True)\n",
    "                set_trainable(model_discriminator, 'discriminator', False)\n",
    "\n",
    "                metrics_srgan = model_srgan.train_on_batch(x_train_lr_batch, [1] * batch_size)\n",
    "            \n",
    "    #             set_trainable(model_generator, 'generator', False)\n",
    "                set_trainable(model_discriminator, 'discriminator', True)\n",
    "\n",
    "                x_train_predictions = model_generator.predict(x_train_lr_batch)\n",
    "                metrics = model_discriminator.train_on_batch(np.concatenate([x_train_predictions, x_train_batch]), \n",
    "                                                             np.concatenate([[0] * batch_size, [1] * batch_size]))\n",
    "                progress.set_description('Srgan: %s: %s; %s: %s; Discriminator: %s: %s; %s: %s' % \n",
    "                    (metrics_names[0], metrics_srgan[0], metrics_names[1], metrics_srgan[1], metrics_names[0], \n",
    "                     metrics[0], metrics_names[1], metrics[1]))\n",
    "                progress.update()\n",
    "\n",
    "#             print(model_srgan.evaluate(x_test_lr_norm, [1] * x_test_lr_norm.shape[0]))\n",
    "#             print(model_generator.evaluate(x_test_lr_norm, x_test_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(model_generator.predict(x_lr[i].reshape(1, h, w, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_srgan.predict(np.zeros((1, 16, 16, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(model_generator.predict(x_lr[i].reshape(1, h, w, 3)).astype(np.uint8)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].reshape(1, 32, 32, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_discriminator.predict(model_generator.predict(x_train_lr[0].reshape(1, 16, 16, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lr[i].reshape(1, h, w, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.predict(x_lr[i].reshape(1, h, w, 3).astype(np.uint8)).astype(np.uint8)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.predict(x_lr[0].reshape(1, h, w, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.predict(x_lr[0].reshape(1, h, w, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
