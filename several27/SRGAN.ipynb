{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import img_as_float\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.utils import conv_utils\n",
    "from keras.engine.topology import Layer\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, PReLU, BatchNormalization, Add, LeakyReLU, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/home/ubuntu/COMP6208-AutoEncoder/several27/data/'\n",
    "path_open_images_560_420 = path_data + 'open_images_560_420/'\n",
    "path_open_images_560_420_train = path_data + 'open_images_560_420_train/'\n",
    "path_open_images_560_420_val = path_data + 'open_images_560_420_val/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(x_train[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare LR CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_resize(images, ratio):\n",
    "    lr_width = images.shape[1] // ratio\n",
    "    lr_height = images.shape[2] // ratio\n",
    "    resized = np.zeros((images.shape[0], lr_width, lr_height, images.shape[3]))\n",
    "    for i in tqdm(range(images.shape[0])):\n",
    "        resized[i] = resize(images[i], (lr_width, lr_height))\n",
    "        \n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_lr = images_resize(x_train, 4)\n",
    "x_test_lr = images_resize(x_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(x_train_lr[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open_images_560_420 generator (or rather 420*560 ugh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_images(path, size=(420, 560, 3), ratio=2, batch_size=32):\n",
    "    lr_height, lr_width = size[0] // ratio, size[1] // ratio\n",
    "    \n",
    "    batch_i = 0\n",
    "    batch = np.zeros((batch_size, size[0], size[1], size[2]))\n",
    "    batch_scaled = np.zeros((batch_size, lr_height, lr_width, size[2]))\n",
    "    \n",
    "    while True:\n",
    "        for file in os.listdir(path):\n",
    "            if not file.endswith('.jpg'):\n",
    "                continue \n",
    "            \n",
    "            if batch_i == batch_size:\n",
    "                yield batch_scaled, batch\n",
    "                \n",
    "                batch_i = 0\n",
    "                batch = np.zeros((batch_size, size[0], size[1], size[2]))\n",
    "                batch_scaled = np.zeros((batch_size, lr_height, lr_width, size[2]))\n",
    "            \n",
    "            file_path = path + file\n",
    "            img = img_as_float(Image.open(file_path))\n",
    "            if len(img.shape) == 2: \n",
    "                 img = np.asarray(np.dstack((img, img, img))) \n",
    "            \n",
    "            batch[batch_i] = img\n",
    "            batch_scaled[batch_i] = resize(img, (lr_height, lr_width))\n",
    "            \n",
    "            batch_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(path):\n",
    "    return sum([1 for file in os.listdir(path) if file.endswith('.jpg')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images(path_open_images_560_420_train), count_images(path_open_images_560_420_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](srgan_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator network\n",
    "\n",
    "1. Input - Image LR\n",
    "2. Conv, kernel 3x3, 64 feature maps, what's s1?\n",
    "3. ParametricReLU\n",
    "4. B=16 residual blocks\n",
    "    1. Conv with 3x3 kernel, 64 feature maps and stride 1\n",
    "    2. Batch Normalization (https://keras.io/layers/normalization/)\n",
    "    3. ParametricReLU\n",
    "    4. Conv with 3x3 kernel, 64 feature maps and stride 1\n",
    "    5. Batch Normalization\n",
    "    6. Elementwise sum (https://keras.io/layers/merge/ add)\n",
    "5. Conv with 3x3 kernel, 64 feature maps and stride 1\n",
    "6. Batch Normalization\n",
    "7. Elementwise sum (https://keras.io/layers/merge/ add)\n",
    "8. Shuffle block x2 (? how it's called ?)\n",
    "    1. Conv with 3x3 kernel, 256 feature maps and stride 1\n",
    "    2. PixelShuffler x2 https://gist.github.com/t-ae/6e1016cc188104d123676ccef3264981\n",
    "    3. ParametricReLU\n",
    "9. Conv with 9x9 kernel, 3 feature maps and stride 1\n",
    "10. Estimated SR (super-resolved) image\n",
    "\n",
    "Is the padding `same` a correct one? Otherwise the dims are not the same\n",
    "Is the number of pixel shufflers the scaling ratio? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://gist.github.com/t-ae/6e1016cc188104d123676ccef3264981\n",
    "\n",
    "class PixelShuffler(Layer):\n",
    "    def __init__(self, size=(2, 2), data_format=None, **kwargs):\n",
    "        super(PixelShuffler, self).__init__(**kwargs)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        if len(input_shape) != 4:\n",
    "            raise ValueError('Inputs should have rank ' +\n",
    "                             str(4) +\n",
    "                             '; Received input shape:', str(input_shape))\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_size, c, h, w = input_shape\n",
    "            if batch_size is None:\n",
    "                batch_size = -1\n",
    "            rh, rw = self.size\n",
    "            oh, ow = h * rh, w * rw\n",
    "            oc = c // (rh * rw)\n",
    "\n",
    "            out = K.reshape(inputs, (batch_size, rh, rw, oc, h, w))\n",
    "            out = K.permute_dimensions(out, (0, 3, 4, 1, 5, 2))\n",
    "            out = K.reshape(out, (batch_size, oc, oh, ow))\n",
    "            return out\n",
    "\n",
    "        elif self.data_format == 'channels_last':\n",
    "            batch_size, h, w, c = input_shape\n",
    "            if batch_size is None:\n",
    "                batch_size = -1\n",
    "            rh, rw = self.size\n",
    "            oh, ow = h * rh, w * rw\n",
    "            oc = c // (rh * rw)\n",
    "\n",
    "            out = K.reshape(inputs, (batch_size, h, w, rh, rw, oc))\n",
    "            out = K.permute_dimensions(out, (0, 1, 3, 2, 4, 5))\n",
    "            out = K.reshape(out, (batch_size, oh, ow, oc))\n",
    "            return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        if len(input_shape) != 4:\n",
    "            raise ValueError('Inputs should have rank ' +\n",
    "                             str(4) +\n",
    "                             '; Received input shape:', str(input_shape))\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            height = input_shape[2] * self.size[0] if input_shape[2] is not None else None\n",
    "            width = input_shape[3] * self.size[1] if input_shape[3] is not None else None\n",
    "            channels = input_shape[1] // self.size[0] // self.size[1]\n",
    "\n",
    "            if channels * self.size[0] * self.size[1] != input_shape[1]:\n",
    "                raise ValueError('channels of input and size are incompatible')\n",
    "\n",
    "            return (input_shape[0],\n",
    "                    channels,\n",
    "                    height,\n",
    "                    width)\n",
    "\n",
    "        elif self.data_format == 'channels_last':\n",
    "            height = input_shape[1] * self.size[0] if input_shape[1] is not None else None\n",
    "            width = input_shape[2] * self.size[1] if input_shape[2] is not None else None\n",
    "            channels = input_shape[3] // self.size[0] // self.size[1]\n",
    "\n",
    "            if channels * self.size[0] * self.size[1] != input_shape[3]:\n",
    "                raise ValueError('channels of input and size are incompatible')\n",
    "\n",
    "            return (input_shape[0],\n",
    "                    height,\n",
    "                    width,\n",
    "                    channels)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'size': self.size,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(PixelShuffler, self).get_config()\n",
    "\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srgan_generator(input_shape, input_=None):\n",
    "    kernel_size = (3, 3)\n",
    "    kernel_size_last = (9, 9)\n",
    "    features = 64\n",
    "    features_shuffle = 256\n",
    "    features_last = 3\n",
    "    B = 16\n",
    "\n",
    "    # 1, 2, 3\n",
    "    input_1 = input_ if input_ is not None else Input(shape=input_shape)\n",
    "    conv2d_2 = Conv2D(filters=features, kernel_size=kernel_size, strides=(1, 1), padding='same')(input_1)\n",
    "    prelu_3 = PReLU()(conv2d_2)\n",
    "\n",
    "    # 4 - residual blocks\n",
    "    last_layer = prelu_3\n",
    "    for _ in range(B):\n",
    "        conv2d_4_A = Conv2D(filters=features, kernel_size=kernel_size, strides=(1, 1), padding='same')(last_layer)\n",
    "        bn_4_B = BatchNormalization()(conv2d_4_A)\n",
    "        prelu_4_C = PReLU()(bn_4_B)\n",
    "        conv2d_4_D = Conv2D(filters=features, kernel_size=kernel_size, strides=(1, 1), padding='same')(prelu_4_C)\n",
    "        bn_4_E = BatchNormalization()(conv2d_4_D)\n",
    "        add_4_F = Add()([last_layer, bn_4_E])\n",
    "\n",
    "        last_layer = add_4_F\n",
    "\n",
    "    # 5, 6, 7\n",
    "    conv2d_5 = Conv2D(filters=features, kernel_size=kernel_size, strides=(1, 1), padding='same')(last_layer)\n",
    "    bn_6 = BatchNormalization()(conv2d_5)\n",
    "    add_7 = Add()([prelu_3, bn_6])\n",
    "\n",
    "    # 8 - shuffle block\n",
    "    last_layer = add_7\n",
    "    for _ in range(1):\n",
    "        conv2d_8_A = Conv2D(filters=features_shuffle, kernel_size=kernel_size, strides=(1, 1), padding='same')(last_layer)\n",
    "        shuffler_8_B = PixelShuffler()(conv2d_8_A)\n",
    "        prelu_8_C = PReLU()(shuffler_8_B)\n",
    "\n",
    "        last_layer = prelu_8_C\n",
    "\n",
    "    # 9 \n",
    "    conv2d_5 = Conv2D(filters=features_last, kernel_size=kernel_size_last, strides=(1, 1), \n",
    "                      padding='same')(last_layer)\n",
    "\n",
    "    model_generator = Model(input_1, conv2d_5)\n",
    "    model_generator.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_generator = srgan_generator((x_train_lr.shape[1], x_train_lr.shape[2], x_train_lr.shape[3]))\n",
    "model_generator = srgan_generator((210, 280, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_version = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='data/srgan_generator_weights_%s.{epoch:03d}_{val_acc:.4f}.hdf5' % train_version, \n",
    "                               verbose=1, save_best_only=False)\n",
    "tb_callback = TensorBoard(log_dir='data/tensorboard/', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_generator.fit(x_train_lr, x_train, epochs=100, validation_data=(x_test_lr, x_test), callbacks=[tb_callback])\n",
    "\n",
    "batch_size = 1\n",
    "n_train = 30 # count_images(path_open_images_560_420_train)\n",
    "n_val = 3 # count_images(path_open_images_560_420_val)\n",
    "with tf.device('/gpu:0'):\n",
    "    model_generator.fit_generator(generator_images(path_open_images_560_420_train, batch_size=batch_size), \n",
    "                                  steps_per_epoch=n_train // batch_size,\n",
    "                                  validation_data=generator_images(path_open_images_560_420_val, \n",
    "                                                                   batch_size=batch_size),\n",
    "                                  validation_steps=n_val // batch_size, epochs=1, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.save('data/srgan_generator_%s.model' % train_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.load_weights('data/srgan_generator_%s.model' % train_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g, type_ in [(generator_images(path_open_images_560_420_train, batch_size=1), 'train'), \n",
    "                 (generator_images(path_open_images_560_420_val, batch_size=batch_size), 'test')]:\n",
    "    plt.figure(type_, figsize=(15, 30))\n",
    "    for i, (x_lr, x) in enumerate(g):\n",
    "        x_lr_ = (x_lr[0] * 255).astype(np.uint8)\n",
    "        x_ = (x[0] * 255).astype(np.uint8)\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 1)\n",
    "        plt.imshow(x_lr_)\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 2)\n",
    "        plt.imshow(x_)\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 3)\n",
    "        plt.imshow((model_generator.predict(x_lr)[0] * 255).astype(np.uint8))\n",
    "\n",
    "        if i >= 9:\n",
    "            break\n",
    "            \n",
    "    plt.savefig('data/srgan_generator_%s_%s.png' % (train_version, type_))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, x_lr type_ in [(x_train, x_train_lr, 'train'), (x_test, x_test_lr, 'test')]:\n",
    "    plt.figure(figsize=(15, 30))\n",
    "    for i in range(10):\n",
    "        plt.subplot(10, 3, (i * 3) + 1)\n",
    "        plt.imshow(x_lr[i])\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 2)\n",
    "        plt.imshow(x[i])\n",
    "\n",
    "        plt.subplot(10, 3, (i * 3) + 3)\n",
    "        h, w = x_lr.shape[1], x_lr.shape[2]\n",
    "        plt.imshow(model_generator.predict(x_lr[i].reshape(1, h, w, 3)).astype(np.uint8)[0])\n",
    "\n",
    "    plt.savefig('data/srgan_generator_%s_%s.png' % (train_version, type_))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 30))\n",
    "for i in range(10):\n",
    "    plt.subplot(10, 3, (i * 3) + 1)\n",
    "    plt.imshow(x_test_lr[i])\n",
    "\n",
    "    plt.subplot(10, 3, (i * 3) + 2)\n",
    "    plt.imshow(x_test[i])\n",
    "\n",
    "    plt.subplot(10, 3, (i * 3) + 3)\n",
    "    w, h = x_test_lr.shape[1], x_test_lr.shape[2]\n",
    "    plt.imshow(model_generator.predict(x_test_lr[i].reshape(1, w, h, 3)).astype(np.uint8)[0])\n",
    "\n",
    "plt.savefig('data/srgan_generator_%s_test.png' % train_version)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator network \n",
    "\n",
    "1. Input image\n",
    "2. Conv2d with 3x3 kernel, 64 filters, and 1 stride\n",
    "3. Leaky ReLU (https://keras.io/layers/advanced-activations/#leakyrelu)\n",
    "3. Conv2d block\n",
    "    1. Conv2d with 3x3 kernel, 64 filters, and 1 stride\n",
    "    2. Batch normalization\n",
    "    3. Leaky ReLU (https://keras.io/layers/advanced-activations/#leakyrelu)\n",
    "4. Repeat 4 with 128 filters x 2 \n",
    "5. Repeat 4 with 256 filters x 2 \n",
    "6. Repeat 4 with 512 filters x 2 \n",
    "7. Dense layer with 1024\n",
    "8. Leaky ReLU\n",
    "9. Dense 1 binary\n",
    "10. Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srgan_discriminator():\n",
    "    input_shape = 32, 32, 3\n",
    "    features_1 = 64\n",
    "    features_2, features_3, features_4 = 128, 256, 512\n",
    "    kernel_size = 3, 3\n",
    "    strides = 1, 1\n",
    "    \n",
    "    input_1 = Input(shape=input_shape)\n",
    "    conv2d_2 = Conv2D(filters=features_1, kernel_size=kernel_size, strides=strides, padding='same')(input_1)\n",
    "    lrelu_3 = LeakyReLU()(conv2d_2)\n",
    "    \n",
    "    conv2d_4_A = Conv2D(filters=features_1, kernel_size=kernel_size, strides=strides, padding='same')(lrelu_3)\n",
    "    conv2d_4_B = BatchNormalization()(conv2d_4_A)\n",
    "    lrelu_4_c = LeakyReLU()(conv2d_4_B)\n",
    "    \n",
    "    last_layer = lrelu_4_c\n",
    "    for _features in [features_2, features_3, features_4]:\n",
    "        for i in range(2):\n",
    "            conv2d_5_A = Conv2D(filters=_features, kernel_size=kernel_size, strides=strides, \n",
    "                                padding='same')(last_layer)\n",
    "            conv2d_5_B = BatchNormalization()(conv2d_5_A)\n",
    "            lrelu_5_C = LeakyReLU()(conv2d_5_B)\n",
    "            \n",
    "            last_layer = lrelu_5_C\n",
    "    \n",
    "    flatten_8 = Flatten()(last_layer)\n",
    "    dense_8 = Dense(1024)(flatten_8)\n",
    "    lrelu_9 = LeakyReLU()(dense_8)\n",
    "    dense_10 = Dense(1, activation='sigmoid')(lrelu_9)\n",
    "    \n",
    "    model_discriminator = Model(input_1, dense_10)\n",
    "    model_discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_discriminator = srgan_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_in = Input(shape=(16, 16, 3))\n",
    "\n",
    "model_generator = srgan_generator((16, 16, 3), input_=generator_in)\n",
    "model_discriminator = srgan_discriminator()\n",
    "\n",
    "generator_out = model_generator(generator_in)\n",
    "\n",
    "discriminator_out = model_discriminator(generator_out)\n",
    "model_srgan = Model(generator_in, discriminator_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        (None, 16, 16, 3)         0         \n",
      "_________________________________________________________________\n",
      "model_19 (Model)             (None, 32, 32, 3)         1736195   \n",
      "_________________________________________________________________\n",
      "model_20 (Model)             (None, 1)                 541565761 \n",
      "=================================================================\n",
      "Total params: 543,301,956\n",
      "Trainable params: 543,294,020\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "srgan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.compile(optimizer='adam', loss='mse')\n",
    "model_discriminator.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_srgan.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fb7f8860c18>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb7f766c9e8>,\n",
       " <keras.layers.advanced_activations.LeakyReLU at 0x7fb7f767c6a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb7f7645a58>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb7f76575c0>,\n",
       " <keras.layers.advanced_activations.LeakyReLU at 0x7fb7f75e8f98>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb7f760dcf8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb7f7599278>,\n",
       " <keras.layers.advanced_activations.LeakyReLU at 0x7fb7f757da58>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb7f758ddd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb7f75302b0>,\n",
       " <keras.layers.advanced_activations.LeakyReLU at 0x7fb7f74fceb8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb7f7510828>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb7f74af160>,\n",
       " <keras.layers.advanced_activations.LeakyReLU at 0x7fb7f7492630>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb7f7421898>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb7f7443240>,\n",
       " <keras.layers.advanced_activations.LeakyReLU at 0x7fb7f7411d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb7f73a56d8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb7f73c64e0>,\n",
       " <keras.layers.advanced_activations.LeakyReLU at 0x7fb7f73264e0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb7f7335f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb7f7335f98>,\n",
       " <keras.layers.advanced_activations.LeakyReLU at 0x7fb7f72a6c18>,\n",
       " <keras.layers.core.Flatten at 0x7fb7f75fb908>,\n",
       " <keras.layers.core.Dense at 0x7fb7f72c8940>,\n",
       " <keras.layers.advanced_activations.LeakyReLU at 0x7fb7f723ce10>,\n",
       " <keras.layers.core.Dense at 0x7fb7f723ccf8>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_discriminator.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
